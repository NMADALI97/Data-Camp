{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpreting linear models\n",
    "==========================\n",
    "\n",
    "Linear models are not that easy to interpret when variables are\n",
    "correlated.\n",
    "\n",
    "See also the [statistics chapter](http://www.scipy-lectures.org/packages/statistics/index.html) of the\n",
    "[scipy lecture notes](http://www.scipy-lectures.org).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data on wages\n",
    "--------------\n",
    "\n",
    "We first download and load some historical data on wages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "# Python 2 vs Python 3:\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "if not os.path.exists('wages.txt'):\n",
    "    # Download the file if it is not present\n",
    "    urlretrieve('http://lib.stat.cmu.edu/datasets/CPS_85_Wages',\n",
    "                'wages.txt')\n",
    "\n",
    "# Give names to the columns\n",
    "names = [\n",
    "    'EDUCATION: Number of years of education',\n",
    "    'SOUTH: 1=Person lives in South, 0=Person lives elsewhere',\n",
    "    'SEX: 1=Female, 0=Male',\n",
    "    'EXPERIENCE: Number of years of work experience',\n",
    "    'UNION: 1=Union member, 0=Not union member',\n",
    "    'WAGE: Wage (dollars per hour)',\n",
    "    'AGE: years',\n",
    "    'RACE: 1=Other, 2=Hispanic, 3=White',\n",
    "    'OCCUPATION: 1=Management, 2=Sales, 3=Clerical, 4=Service, 5=Professional, 6=Other',\n",
    "    'SECTOR: 0=Other, 1=Manufacturing, 2=Construction',\n",
    "    'MARR: 0=Unmarried,  1=Married',\n",
    "]\n",
    "\n",
    "short_names = [n.split(':')[0] for n in names]\n",
    "data = pandas.read_csv('wages.txt', skiprows=27, skipfooter=6, sep=None,\n",
    "                       header=None, engine='python')\n",
    "data.columns = short_names\n",
    "\n",
    "# Log-transform the wages, as they typically increase with\n",
    "# multiplicative factors\n",
    "import numpy as np\n",
    "data['WAGE'] = np.log10(data['WAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge of correlated features\n",
    "--------------------------------------------\n",
    "\n",
    "Plot scatter matrices highlighting the links between different\n",
    "variables measured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "# The simplest way to plot a pairplot\n",
    "seaborn.pairplot(data, vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE', 'SEX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fancier pair plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "g = seaborn.PairGrid(data,\n",
    "                     vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE', 'SEX'],\n",
    "                     diag_sharey=False)\n",
    "g.map_upper(seaborn.kdeplot)\n",
    "g.map_lower(plt.scatter, s=2)\n",
    "g.map_diag(seaborn.kdeplot, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that age and experience are highly correlated\n",
    "\n",
    "A link between a single feature and the target is a *marginal* link.\n",
    "\n",
    "\n",
    "Univariate feature selection selects on marginal links.\n",
    "\n",
    "Linear model compute *conditional* links: removing the effects of other\n",
    "features on each feature. This is hard when features are correlated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients of a linear model\n",
    "--------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "features = [c for c in data.columns if c != 'WAGE']\n",
    "X = data[features]\n",
    "y = data['WAGE']\n",
    "ridge = linear_model.RidgeCV()\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Visualize the coefs\n",
    "coefs = ridge.coef_\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(np.arange(coefs.size), coefs)\n",
    "plt.yticks(np.arange(coefs.size), features)\n",
    "plt.title(\"Coefficients\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling coefficients**: coefs cannot easily be compared if X is not\n",
    "standardized: they should be normalized to the variance of X: the\n",
    "greater the variance of a feature, the large the impact of the\n",
    "corresponding coefficent on the output.\n",
    "\n",
    "If the different features have differing, possibly arbitrary, scales,\n",
    "then scaling the coefficients by the feature scale often helps\n",
    "interpretation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = X.std()\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(np.arange(coefs.size), coefs * X_std)\n",
    "plt.yticks(np.arange(coefs.size), features)\n",
    "plt.title(\"Scaled coefficients\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the age and experience can be better compared: and experience does\n",
    "appear as more important than age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When features are not too correlated and there is plenty of samples,\n",
    "this is the well-known regime of standard statistics in linear models.\n",
    "Machine learning is not needed, and statsmodels is a great tool (see the\n",
    "[statistics chapter in scipy-lectures](http://www.scipy-lectures.org/packages/statistics/index.html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of regularization\n",
    "--------------------------------------------\n",
    "\n",
    "Sparse models use l1 regularization to puts some variables to\n",
    "zero. This can often help interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.LassoCV(max_iter=100000, cv=5)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "coefs = lasso.coef_\n",
    "plt.barh(np.arange(coefs.size), coefs * X_std)\n",
    "plt.yticks(np.arange(coefs.size), features)\n",
    "plt.title(\"Sparse coefficients\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two variables are very correlated (such as age and experience), it\n",
    "will put arbitrarily one or the other to zero depending on their SNR.\n",
    "\n",
    "Here we can see that age probably overshadowed experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability to gauge significance\n",
    "--------------------------------\n",
    "\n",
    "Stability of coefficients when perturbing the data helps giving an\n",
    "informal evaluation of the significance of the coefficients. Note that\n",
    "this is not significance testing in the sense of p-values, as a model\n",
    "that returns coefficients always at one indepently of the data will\n",
    "appear as very stable though it clearly does not control for false\n",
    "detections.\n",
    "\n",
    "We can do this in a cross-validation loop, using the argument\n",
    "\"return_estimator\" of `sklearn.model_selection.cross_validate`\n",
    "which has been added in version 0.20 of scikit-learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the lasso estimator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lasso = cross_validate(lasso, X, y, return_estimator=True, cv=10)\n",
    "coefs_ = [estimator.coef_ for estimator in cv_lasso['estimator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results with seaborn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ = pandas.DataFrame(coefs_, columns=features) * X_std\n",
    "plt.figure(figsize=(6, 4))\n",
    "seaborn.boxplot(data=coefs_, orient='h')\n",
    "seaborn.stripplot(data=coefs_, orient='h', color='k')\n",
    "plt.axvline(x=0, color='.5')  # Add a vertical line at 0\n",
    "plt.title('Sparse coefficients')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the ridge estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge = cross_validate(ridge, X, y, return_estimator=True, cv=10)\n",
    "coefs_ = [estimator.coef_ for estimator in cv_ridge['estimator']]\n",
    "\n",
    "coefs_ = pandas.DataFrame(coefs_, columns=features) * X_std\n",
    "plt.figure(figsize=(6, 4))\n",
    "seaborn.boxplot(data=coefs_, orient='h')\n",
    "seaborn.stripplot(data=coefs_, orient='h', color='k')\n",
    "plt.axvline(x=0, color='.5') # Add a vertical line at 0\n",
    "plt.title('Non-sparse coefficients (ridge)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story is different. Note also that lasso coefficients are much more\n",
    "instable than the ridge. It is often the case that sparse models are\n",
    "unstable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is the truth?\n",
    "\n",
    "\n",
    "Note the difference between the lasso and the ridge estimator: we do\n",
    "not have enough data to perfectly estimate conditional relationships,\n",
    "hence the prior (ie the regularization) makes a difference, and its is\n",
    "hard to tell from the data which is the \"truth\".\n",
    "\n",
    "One reasonnable model-selection criterion is to believe most the model\n",
    "that predicts best. For this, we can inspect the prediction scores\n",
    "obtained via the cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pandas.DataFrame({'lasso': cv_lasso['test_score'],\n",
    "                           'ridge': cv_ridge['test_score']})\n",
    "plt.figure(figsize=(6, 2))\n",
    "seaborn.boxplot(data=scores, orient='h')\n",
    "seaborn.stripplot(data=scores, orient='h', color='k')\n",
    "plt.title(\"Model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the limitations of cross-validation explained previously\n",
    "still apply. Ideally, we should use a ShuffleSplit cross-validation\n",
    "object to sample many times and have a better estimate of the\n",
    "posterior, both for the coefficients and the test scores.\n",
    "\n",
    "#### Conclusion on factors of wages?\n",
    "\n",
    "\n",
    "As always, concluding is hard. That said, it seems that we should\n",
    "prefer the scaled ridge coefficients.\n",
    "\n",
    "Hence, the most important factors of wage are education and\n",
    "experience, followed by sex: at the same education and experience\n",
    "females earn less than males. Note that this last statement is a\n",
    "statement about the link between wage and sex, conditional on education\n",
    "and experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpreting random forests\n",
    "=======================\n",
    "\n",
    "Interpreting random forests can be done through feature importance. This index explain how much a feature is used to split a tree. This is also known as the Gini importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "forest = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "forest.fit(X, y)\n",
    "\n",
    "# Visualize the feature importance\n",
    "importance = forest.feature_importances_\n",
    "from matplotlib import pyplot as plt\n",
    "plt.barh(np.arange(importance.size), importance)\n",
    "plt.yticks(np.arange(importance.size), features)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importances are a proxy for the mutual information between the\n",
    "feature and the target, conditionally on the other features. The\n",
    "conditioning is difficult and not well controlled.\n",
    "\n",
    "Higher-cardinality categorical variables will have larger\n",
    "feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import per"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
