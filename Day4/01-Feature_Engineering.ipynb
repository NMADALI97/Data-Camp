{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Author : [Alexandre Gramfort](http://alexandre.gramfort.net)\n",
    "         \n",
    "         \n",
    "with some code snippets from [Olivier Grisel](http://ogrisel.com/) (leaf encoder)\n",
    "\n",
    "It is the most creative aspect of Data Science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data contains both quantitative and categorical variables. These categorical have some predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, x='pclass', y='survived', hue='sex', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is how to feed these non-quantitative features to a supervised learning model?\n",
    "\n",
    "## Categorical features\n",
    "\n",
    " - Nearly always need some treatment\n",
    " - High cardinality can create very sparse data\n",
    " - Difficult to impute missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "\n",
    " - It is the most basic method. It is used with most linear algorithms\n",
    " - Drop first column to avoid collinearity\n",
    " - It uses sparse format which is memory-friendly\n",
    " - Most current implementations don’t gracefully treat missing, unseen variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['embarked']]\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OneHotEncoder().fit_transform(df1.head(10)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder().fit_transform(df1).toarray()  # problem of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid colinearity\n",
    "OneHotEncoder(drop='first').fit_transform(df1.head(10)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal encoding\n",
    "\n",
    " - Give every categorical variable a unique numerical ID\n",
    " - Useful for non-linear tree-based algorithms (forests, gradient-boosting)\n",
    " - Does not increase dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "OrdinalEncoder().fit_transform(df1.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count encoding\n",
    "\n",
    "Replace categorical variables with their count in the train set\n",
    "\n",
    "- Useful for both linear and non-linear algorithms\n",
    "- Can be sensitive to outliers\n",
    "- May add log-transform, works well with counts\n",
    "- Replace unseen variables with `1`\n",
    "- May give collisions: same encoding, different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.CountEncoder().fit_transform(df1.head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label / Ordinal count encoding\n",
    "\n",
    "Rank categorical variables by count in train set\n",
    "\n",
    "- Useful for both linear and non-linear algorithms\n",
    "- Not sensitive to outliers\n",
    "- Won’t give same encoding to different variables\n",
    "- Best of both worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "class CountOrdinalEncoder(OrdinalEncoder):\n",
    "    \"\"\"Encode categorical features as an integer array\n",
    "    usint count information.\n",
    "    \"\"\"\n",
    "    def __init__(self, categories='auto', dtype=np.float64):\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the OrdinalEncoder to X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to determine the categories of each feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        super().fit(X)\n",
    "        X_list, _, _ = self._check_X(X)\n",
    "        # now we'll reorder by counts\n",
    "        for k, cat in enumerate(self.categories_):\n",
    "            counts = []\n",
    "            for c in cat:\n",
    "                counts.append(np.sum(X_list[k] == c))\n",
    "            order = np.argsort(counts)\n",
    "            self.categories_[k] = cat[order]\n",
    "        return self\n",
    "\n",
    "coe = CountOrdinalEncoder()\n",
    "coe.fit_transform(pd.DataFrame(['fr', 'fr', 'fr', 'en', 'en', 'es', 'es']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash encoding\n",
    "\n",
    "Does “OneHot-encoding” with arrays of a fixed length.\n",
    "\n",
    "- Avoids extremely sparse data\n",
    "- May introduce collisions\n",
    "- Can repeat with different hash functions and bag result for small bump in accuracy\n",
    "- Collisions usually degrade results, but may improve it.\n",
    "- Gracefully deals with new variables (eg: new user-agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.hashing.HashingEncoder(n_components=4).fit_transform(df1.head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding\n",
    "\n",
    "Encode categorical variables by their ratio of target (binary classification or regression)\n",
    "\n",
    "Formula reads:\n",
    "\n",
    "$$\n",
    "    TE(X) = \\alpha(n(X)) E[ y | x=X ] +  (1 - \\alpha(n(X))) E[y]\n",
    "$$\n",
    "\n",
    "where $n(X)$ is the count of category $X$ and $\\alpha$ is a monotonically increasing function bounded between 0 and 1.[1].\n",
    "\n",
    "- Add smoothing to avoid setting variable encodings to 0.\n",
    "```\n",
    "[1] Micci-Barreca, 2001: A preprocessing scheme for\n",
    "high-cardinality categorical attributes in classification\n",
    "and prediction problems.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dirty_cat as dc  # install with: pip install dirty_cat\n",
    "\n",
    "X = np.array(['A', 'B', 'C', 'A', 'B', 'B'])[:, np.newaxis]\n",
    "y = np.array([1  , 1  , 1  , 0  , 0  , 1])\n",
    "\n",
    "dc.TargetEncoder(clf_type='binary-clf').fit_transform(X, y)\n",
    "# If \\alpha was 1 you would get: [0.5, 0.66, 1, 0.5, 0.66, 0.66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN encoding\n",
    "\n",
    "The idea to encode that a feature was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('survived').count()[['deck', 'sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = np.array([0, 1., np.nan, 2., 0.])[:, None]\n",
    "SimpleImputer(strategy='median', add_indicator=True).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "X = np.array([0, 1., np.nan, 2., 0.])[:, None]\n",
    "MissingIndicator().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial encoding\n",
    "\n",
    "Encode interactions between categorical variables\n",
    "\n",
    "- Linear algorithms without interactions can not solve the XOR problem\n",
    "- A polynomial kernel *can* solve XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [1, 1], [1, 0], [0, 0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "PolynomialFeatures(include_bias=False, interaction_only=True).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To go beyond\n",
    "\n",
    "You can also use some form of embedding eg using a Neural Network to create dense embeddings from categorical variables.\n",
    "\n",
    "- Map categorical variables in a function approximation problem into Euclidean spaces\n",
    "- Faster model training.\n",
    "- Less memory overhead.\n",
    "- Can give better accuracy than 1-hot encoded.\n",
    "- See for example https://arxiv.org/abs/1604.06737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning\n",
    "\n",
    "See https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.randn(10, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KBinsDiscretizer(n_bins=2).fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "\n",
    "Scale to numerical variables into a certain range\n",
    "\n",
    "- Standard (Z) Scaling\n",
    "- MinMax Scaling\n",
    "- Root scaling\n",
    "- Log scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = 10 + rng.randn(10, 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "X = np.arange(1, 10)[:, np.newaxis]\n",
    "FunctionTransformer(func=np.log).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf coding\n",
    "\n",
    "The following is an implementation of a trick found in:\n",
    "\n",
    "Practical Lessons from Predicting Clicks on Ads at Facebook\n",
    "Junfeng Pan, He Xinran, Ou Jin, Tianbing XU, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, Joaquin Quiñonero Candela\n",
    "International Workshop on Data Mining for Online Advertising (ADKDD)\n",
    "\n",
    "https://www.facebook.com/publications/329190253909587/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "class TreeTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"One-hot encode samples with an ensemble of trees\n",
    "    \n",
    "    This transformer first fits an ensemble of trees (e.g. gradient\n",
    "    boosted trees or a random forest) on the training set.\n",
    "\n",
    "    Then each leaf of each tree in the ensembles is assigned a fixed\n",
    "    arbitrary feature index in a new feature space. If you have 100\n",
    "    trees in the ensemble and 2**3 leafs per tree, the new feature\n",
    "    space has 100 * 2**3 == 800 dimensions.\n",
    "    \n",
    "    Each sample of the training set go through the decisions of each tree\n",
    "    of the ensemble and ends up in one leaf per tree. The sample if encoded\n",
    "    by setting features with those leafs to 1 and letting the other feature\n",
    "    values to 0.\n",
    "    \n",
    "    The resulting transformer learn a supervised, sparse, high-dimensional\n",
    "    categorical embedding of the data.\n",
    "    \n",
    "    This transformer is typically meant to be pipelined with a linear model\n",
    "    such as logistic regression, linear support vector machines or\n",
    "    elastic net regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.fit_transform(X, y)\n",
    "        return self\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        self.estimator_ = clone(self.estimator)\n",
    "        self.estimator_.fit(X, y)\n",
    "        self.binarizers_ = []\n",
    "        sparse_applications = []\n",
    "        estimators = np.asarray(self.estimator_.estimators_).ravel()\n",
    "        for t in estimators:\n",
    "            lb = LabelBinarizer(sparse_output=True)\n",
    "            X_leafs = t.tree_.apply(X.astype(np.float32))\n",
    "            sparse_applications.append(lb.fit_transform(X_leafs))\n",
    "            self.binarizers_.append(lb)\n",
    "        return hstack(sparse_applications)\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        sparse_applications = []\n",
    "        estimators = np.asarray(self.estimator_.estimators_).ravel()\n",
    "        for t, lb in zip(estimators, self.binarizers_):\n",
    "            X_leafs = t.tree_.apply(X.astype(np.float32))\n",
    "            sparse_applications.append(lb.transform(X_leafs))\n",
    "        return hstack(sparse_applications)\n",
    "\n",
    "\n",
    "boosted_trees = GradientBoostingClassifier(\n",
    "    max_leaf_nodes=5, learning_rate=0.1,\n",
    "    n_estimators=10, random_state=0,\n",
    ")\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "TreeTransform(boosted_trees).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Limiting yourself to LogisticRegression propose features to predict survival.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = df.survived.values\n",
    "X = df.drop(['survived', 'alive'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "ct = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), ['age', 'pclass', 'fare'])\n",
    ")\n",
    "clf = make_pipeline(ct, lr)\n",
    "np.mean(cross_val_score(clf, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do better !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
